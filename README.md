# LLMLayer JavaScriptÂ SDK

[![npm](https://img.shields.io/npm/v/llmlayer?color=blue)](https://www.npmjs.com/package/llmlayer)
[![license](https://img.shields.io/npm/l/llmlayer.svg)](LICENSE)

> **SearchÂ â€“Â ReasonÂ â€“Â Cite** with one function call
> The *official* JavaScriptÂ /Â TypeScript client for the **[LLMLayer SearchÂ &Â AnswerÂ API](https://llmlayer.ai)**.

---

## âœ¨Â Features

| Â                            | Â                                                       |
| --------------------------- | ------------------------------------------------------ |
| ğŸ—‚Â **Multiâ€‘provider**       | OpenAI, Groq, DeepSeek       |
| ğŸ”„Â **BlockingÂ &Â Streaming** | Promiseâ€‘based blocking call **or** asyncâ€‘iterator stream |
| â±Â **SSE chunks**            | Lowâ€‘latency output perfect for chatÂ UIs & CLIs         |
| ğŸ›¡Â **Typed errors**         | `InvalidRequest`, `ProviderError`, `RateLimitError`, â€¦ |

---

## Installation

```bash
npm i llmlayer            #Â or  yarn add llmlayer  /  pnpm add llmlayer
```

*Requires NodeÂ **â‰¥Â 16** (works in browsers & Workers too).*
*(On NodeÂ â‰¤Â 18 the SDK autoâ€‘loads `undici` to provideÂ `fetch`.)*

---

## QuickÂ Start

```ts
import { LLMLayerClient } from 'llmlayer';

const client = new LLMLayerClient({
  apiKey:      process.env.LLMLAYER_API_KEY,   //Â LLMLayer bearer
});

/*Â 1ï¸âƒ£Â BlockingÂ callÂ */
const resp = await client.search({
  query: 'Why is the sky blue?',
  model: 'openai/gptâ€‘4.1â€‘mini',
  returnSources: true
});
console.log(resp.llm_response);

/*Â 2ï¸âƒ£Â StreamingÂ */
for await (const ev of client.searchStream({
  query: 'Explain brown dwarfs in two sentences',
  model: 'groq/kimi-k2',
  returnSources: true
})) {
  if (ev.type === 'llm')        process.stdout.write(ev.content);
  else if (ev.type === 'sources') console.log('\nSources:', ev.data);
  else if (ev.type === 'done')    console.log(`\nâœ“ finished in ${ev.response_time}s`);
}
```

> **TipÂ â€” use envÂ vars in production** to keep keys out of source control.

---

## BlockingÂ vsÂ Streaming

| Method           | Returns                             | When to use                         |
| ---------------- | ----------------------------------- | ----------------------------------- |
| `search()`       | `Promise<SimplifiedSearchResponse>` | Quick oneâ€‘shot requests             |
| `searchStream()` | `AsyncGenerator<Event>`             | Realâ€‘time progress for long answers |

---

## API Reference

### `new LLMLayerClient(options)`

| option        | type          | default                    | description                        |
| ------------- | ------------- | -------------------------- | ---------------------------------- |
| `apiKey`      | `string`      | â€”                          | **Required** LLMLayer bearer token |
| `baseURL`     | `string`      | `https://api.llmlayer.dev` | Override for selfâ€‘host/staging     |
| `timeoutMs`   | `number`      | `60000`                    | Abort afterÂ Nâ€¯ms                   |

#### Methods

| method                 | description                                |
| ---------------------- | ------------------------------------------ |
| `search(params)`       | Blocking call â†’ `SimplifiedSearchResponse` |
| `searchStream(params)` | Async iterator yielding SSE events         |

Type defs live in [`src/models.ts`](./src/models.ts).

---

## KeyÂ RequestÂ Parameters

| name                | type                 | default      | notes                                               |
|---------------------|----------------------|--------------|-----------------------------------------------------|
| `query`             | `string`             | â€”            | Your question                                       |
| `model`             | `string`             | â€”            | Provider model name                                 |
| `returnSources`     | `boolean`            | `false`      | Attach sources list                                 |
| `returnImages`      | `boolean`            | `false`      | Include image results                               |
| `answerType`        | `'markdown'\|â€¦`      | `'markdown'` | `json` returns structured output                    |
| `searchType`        | `'general'\| 'news'` | `'general'`  | Vertical bias                                       |
| `responseLanguage`  | `string`             | `'auto'`     | e.g. `'en'`, `'fr'`, or `'auto'` detection          |
| `location`          | `string`             | `'us'`       | Geo bias for web search (ISOâ€‘2 country code)        |
| `dateFilter`        | `'hour'â€¦'anytime'`   | `'anytime'`  | Recency filter                                      |
| `domainFilter`      | `string[]`           | â€”            | `['nytimes.com', '-wikipedia.org']`                 |
| `maxQueries`        | `number`             | `1`          | How many search queries LLMLayer should generate    |
| `maxTokens`         | `number`             | `1500`       | LLM response length                                 |
| `temperature`       | `number`             | `0.7`        | Creativity knob                                     |
| `searchContextSize` | `string`             | `medium`     | values : `low`  `medium`  `high` |


See the **Parameters** page in the docs site for the full table.
\*\* page in the docs site for the full table.

---

## Environment Variables

```bash
#Â LLMLayer bearer (required)
export LLMLAYER_API_KEY="llm_xxxxxxxxxxxxx"
```

Then simply:

```ts
const client = new LLMLayerClient({ }); //Â all keys autoâ€‘picked from env
```

---

## NeedÂ Help?

* ğŸ’¬Â [Join our Discord](https://discord.gg/EqQF4cjTq5)
* ğŸ›Â [Open an issue](https://github.com/YassKhazzan/llmlayer_js/issues)

---

MITÂ Â©Â 2025Â LLMLayerÂ Inc.
